{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "RecoNet",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyMsPb18J+VVYqW4FwqnvGrw",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/shanyaanand/RecoNet/blob/master/RecoNet.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "axZmhr5Vjsxu",
        "colab_type": "code",
        "outputId": "fba57e51-01c5-44c8-c830-d6d434fb7728",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-d7QrJvJZ24i",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from google.colab import files\n",
        "files.upload()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q-4ISqtrj72L",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "import matplotlib.pyplot as plt\n",
        "import torch.nn as nn\n",
        "from torch.autograd import Variable\n",
        "import torch.optim as optim\n",
        "from sklearn.model_selection import train_test_split\n",
        "import torch.nn.functional as F\n",
        "import pandas as pd \n",
        "import numpy as np\n",
        "from sklearn.metrics import recall_score\n",
        "from torch.utils.data import Dataset\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fdT7pDEGR0gn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "class Norm(nn.Module):\n",
        "    def __init__(self, d_model, eps = 1e-6):\n",
        "        super(Norm, self).__init__()\n",
        "        self.size = d_model\n",
        "        # create two learnable parameters to calibrate normalisation\n",
        "        self.alpha = nn.Parameter(torch.ones(self.size))\n",
        "        self.bias = nn.Parameter(torch.zeros(self.size))\n",
        "        self.eps = eps\n",
        "    def forward(self, x):\n",
        "        norm = self.alpha * (x - x.mean(dim=-1, keepdim=True)) \\\n",
        "        / (x.std(dim=-1, keepdim=True) + self.eps) + self.bias\n",
        "        return norm\n",
        "\n",
        "def adjust_learning_rate(optimizer, lr, lr_decay):\n",
        "\n",
        "    for group in optimizer.param_groups:\n",
        "        if 'step' not in group:\n",
        "            group['step'] = 0\n",
        "        group['step'] += 1\n",
        "\n",
        "        group['lr'] = lr / (1 + group['step'] * lr_decay)\n",
        "\n",
        "\n",
        "def create_optimizer(model, new_lr, opt, lr_decay, wd):\n",
        "    # setup optimizer\n",
        "    if opt == 'sgd':\n",
        "        optimizer = optim.SGD(model.parameters(), lr=new_lr,\n",
        "                              momentum=0.9, dampening=0.9,\n",
        "                              weight_decay=args.wd)\n",
        "    elif opt == 'adam':\n",
        "        optimizer = optim.Adam(model.parameters(), lr=new_lr,\n",
        "                               weight_decay=wd)\n",
        "    elif opt == 'adagrad':\n",
        "        optimizer = optim.Adagrad(model.parameters(),\n",
        "                                  lr=new_lr,\n",
        "                                  lr_decay=lr_decay,\n",
        "                                  weight_decay=wd)\n",
        "    return optimizer\n",
        "\n",
        "\n",
        "def Filtering(ratings, movies, threshold = 50):\n",
        "        \n",
        "    count = {} \n",
        "    for i in ratings:\n",
        "        key = i[1]\n",
        "        if key in count:\n",
        "            count[key] +=1\n",
        "        else:\n",
        "            count[key] = 1\n",
        "\n",
        "    fast = {}\n",
        "    for i in movies:\n",
        "        key = i[0]\n",
        "        fast[key] = i\n",
        "\n",
        "    Train_data = []\n",
        "    Test_data = []\n",
        "    append = {}\n",
        "    genres = []\n",
        "    for i in ratings:\n",
        "        key = i[1]\n",
        "        if count[key] > threshold:\n",
        "            if key in append:\n",
        "                if np.random.rand() >= 0.33:\n",
        "                  Train_data.append(i)\n",
        "                else:\n",
        "                  Test_data.append(i)\n",
        "\n",
        "            else:\n",
        "                append[key] = 1\n",
        "                Train_data.append(i)\n",
        "                genres.append(fast[key])\n",
        "\n",
        "    temp = indxing(Train_data, Test_data, genres)\n",
        "    return temp\n",
        "\n",
        "def indxing(Train_list, Test_list, genres_list):\n",
        "    \"\"\"\n",
        "        movie id in the dataset contains gap in between, thus this function map gap id to ungap id \n",
        "    \"\"\"\n",
        "    m_index = {}\n",
        "    for i in range(len(genres_list)):\n",
        "        m_index[genres_list[i][0]] = i + 1\n",
        "\n",
        "    train_data = []\n",
        "    for i in Train_list:\n",
        "        temp = i\n",
        "        temp[1] = m_index[i[1]]\n",
        "        train_data.append(temp)\n",
        "\n",
        "    test_data = []\n",
        "    for i in Test_list:\n",
        "        temp = i\n",
        "        temp[1] = m_index[i[1]]\n",
        "        test_data.append(temp)\n",
        "\n",
        "    movie = []\n",
        "    for i in range(len(genres_list)):\n",
        "        temp = genres_list[i]\n",
        "        temp[0] = m_index[genres_list[i][0]]\n",
        "        movie.append(temp) \n",
        "\n",
        "    return [np.array(train_data), np.array(test_data), np.array(movie)]\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2qlYs_9NjYlo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "movie_genre = {\"Action\" : 1, \n",
        "    \"Adventure\" : 2,\n",
        "    \"Animation\" : 3,\n",
        "    \"Children\" : 4,\n",
        "    \"Comedy\" : 5,\n",
        "    \"Crime\" : 6,\n",
        "    \"Documentary\" : 7,\n",
        "    \"Drama\" : 8,\n",
        "    \"Fantasy\" : 9,\n",
        "    \"Film-Noir\" : 10, \n",
        "    \"Horror\" : 11,\n",
        "    \"Musical\" : 12,\n",
        "    \"Mystery\" : 13,\n",
        "    \"Romance\" : 14,\n",
        "    \"Sci-Fi\" : 15,\n",
        "    \"Thriller\" : 16,\n",
        "    \"War\" : 17,\n",
        "    \"Western\" : 18\n",
        "  }\n",
        "rating = np.array(pd.read_csv(\"ratings.csv\"))\n",
        "genres = np.array(pd.read_csv(\"movies.csv\"))\n",
        "temp = Filtering(rating, genres)\n",
        "TrainData, TestData, genres = temp[0], temp[1], temp[2]\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FOBgwDowVsiE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Emdedding(nn.Module):\n",
        "\tdef __init__(self, hidden_size, input_size, output_size):\n",
        "\t\tsuper(Emdedding, self).__init__()\n",
        "\t\tself.lookup = nn.Linear(input_size, hidden_size,bias=False)\n",
        "\t\tself.prediction = nn.Linear(hidden_size, output_size, bias=False)\n",
        "\t\tself.relu = nn.ReLU()\n",
        "\t\tself.BN = nn.BatchNorm1d(hidden_size)\n",
        "\t\tself.Drop = nn.Dropout(0.33)\n",
        "\tdef forward(self, x):\n",
        "\n",
        "\t\t# x_aux = self.Drop(self.relu(self.BN(self.lookup(x))))\n",
        "\t\tx_aux = (self.lookup(x))\\\n",
        "\t\t\t/torch.abs(x.sum(-1).view(x.size()[0], 1) + 1e-8*torch.ones([x.size()[0], 1]).cuda())\n",
        "\t\tx = (self.prediction(x_aux))\n",
        "\t\treturn x, x_aux\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PDTzXqoNjNav",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "class dataloader(Dataset):\n",
        "\t\n",
        "\tdef __init__(self, rating_list, genres_list, movie_genre, mode = \"Train\"):\n",
        "\t\t\"\"\"\n",
        "\t\t\tArgs : numpy array of rating, numpy array of movies, dict. of movie genre to integer, mode of operation\n",
        "\t\t\"\"\"\n",
        "\t\tself.rating_list = rating_list\n",
        "\t\tself.genres_list = genres_list\n",
        "\t\tself.movie_genre = movie_genre\n",
        "\t\tself.length = rating_list.shape[0]\n",
        "\t\tself.n_users = len(np.unique(rating_list[:, 0]))# number of unique users\n",
        "\t\tself.n_movies = len(genres_list)# number of movies\n",
        "\t\tself.n_attr = len(movie_genre) + 1# number of features\n",
        "\t\tself.un_user = np.unique(rating_list[:, 0])\n",
        "\n",
        "\tdef UserData(self, user):\n",
        "\t\t\"\"\"\n",
        "\t\t\tArgs : user id \n",
        "\t\t\tOutput : input vector(as mention in the paper) of dimension = #movies * #features\n",
        "\t\t\"\"\"\n",
        "\t\tphi_v = [0]*self.n_attr# attributes feature\n",
        "\t\tVv = [0]*self.n_movies# movies feature\n",
        "\t\tidx = np.where(self.rating_list[:, 0] == user)[0]\n",
        "\t\tmovies = self.rating_list[idx, 1]# list of movies he/she rated\n",
        "\t\tstars = self.rating_list[idx, 2]\n",
        "\t\tfor k in range(len(movies)):\n",
        "\t\t\tmovie = movies[k]\n",
        "\t\t\tstar = stars[k]\n",
        "\t\t\tif star >= 0.0:\n",
        "\t\t\t\tVv[int(movie) - 1] += 1 #  movie is 1 index\n",
        "\t\t\tx = np.where(self.genres_list[:, 0] == movie)[0]\n",
        "\t\t\tstrings = self.genres_list[x, 2][0].split(\"|\")\n",
        "\t\t\tfor string in strings:\n",
        "\t\t\t\tif string in self.movie_genre:\n",
        "\t\t\t\t\tphi_v[self.movie_genre[string] - 1] = 1 # movie_genre is 1 index\n",
        "\t\t\t\telse:\n",
        "\t\t\t\t\tphi_v[18] = 1\n",
        "\n",
        "\t\treturn np.concatenate((Vv, phi_v), axis=0)\n",
        "\n",
        "\tdef __len__(self):\n",
        "\t\treturn self.length\n",
        "\t\n",
        "\tdef __getitem__(self, idx):\n",
        "\n",
        "\t\tX = self.UserData(self.rating_list[idx, 0])\n",
        "\t\ty = [int(self.rating_list[idx, 1] - 1)]# remove the movie\n",
        "\t\t#remove features of the movie\n",
        "\t\t# w = np.where(self.genres_list[:, 0]== self.rating_list[idx, 1])[0]\n",
        "\t\tw = self.rating_list[idx, 1] - 1\n",
        "\t\tstrings = self.genres_list[int(w), 2].split(\"|\")\t\n",
        "\t\tfor string in strings:\n",
        "\t\t\tif string in self.movie_genre:\n",
        "\t\t\t\tt = self.movie_genre[string] - 20\n",
        "\t\t\t\tX[t] -= 1\n",
        "\t\t\telse:\n",
        "\t\t\t\tX[-1] -= 1\n",
        "\t\tX[y] = 0\n",
        "\t\tsample = {'data' : torch.Tensor(X), 'label' : torch.Tensor(y)}\n",
        "\t\treturn sample\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d5HvIbDijKeW",
        "colab_type": "code",
        "outputId": "906e3065-164a-4f5e-a2b8-cc2c505c5b9c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "kwargs = {'num_workers': 8, 'pin_memory': True} \n",
        "print(\"number of training data {} nummber of users {} number of movies {}\".format(len(TrainData), len(np.unique(TrainData[:, 0])), len(np.unique(TrainData[:, 1]))))\n",
        "print(\"number of testing data {} nummber of users {} number of movies {}\".format(len(TestData), len(np.unique(TestData[:, 0])), len(np.unique(TestData[:, 1]))))\n",
        "TrainLoader = dataloader(TrainData, genres, movie_genre)\n",
        "TestLoader = dataloader(TestData, genres, movie_genre) \t \t"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "number of training data 27400 nummber of users 605 number of movies 436\n",
            "number of testing data 13260 nummber of users 596 number of movies 436\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-3LlZltJi6mx",
        "colab_type": "code",
        "outputId": "485cb840-26fc-469e-f6ca-9a389e6502a5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 612
        }
      },
      "source": [
        "net = Emdedding(1024, len(genres)+len(movie_genre)+1, len(genres))\n",
        "net.cuda()\n",
        "# import os \n",
        "# if os.path.isfile('/content/drive/My Drive/checkpoint_100000.pth'):\n",
        "# \tprint('=> loading checkpoint {}'.format('/content/drive/My Drive/checkpoint_100000.pth'))\n",
        "# \tcheckpoint = torch.load('/content/drive/My Drive/checkpoint_100000.pth')\n",
        "# \tnet.load_state_dict(checkpoint['state_dict'])\n",
        "# else:\n",
        "# \tprint(\"cannot find file\")\n",
        "# print(\"loading done\")\n",
        "batch_size = 64*4\n",
        "epoch = 50\n",
        "lr = 0.01\n",
        "lr_decay = 0.01\n",
        "wt_decay = 0.0\n",
        "\n",
        "# optimizer = create_optimizer(net, lr, \"adagrad\", lr_decay, wt_decay)\n",
        "optimizer = create_optimizer(net, lr, \"adam\", lr_decay, wt_decay)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "metric_test = []\n",
        "metric_train = []\n",
        "TrainSample = torch.utils.data.DataLoader(TrainLoader, batch_size= batch_size, shuffle=True)\n",
        "TestSample = torch.utils.data.DataLoader(TestLoader, batch_size= 8, shuffle=False)\n",
        "\n",
        "print(\"starting training!\")\n",
        "for n_epoch in range(epoch):\n",
        "\terror = 0\n",
        "\tlabels = []\n",
        "\tlogits = []\n",
        "\tcount = 0\n",
        "\tnet.eval()\n",
        "\tfor i, batch in enumerate(TestSample):\n",
        "\t\tX = Variable(batch['data'].cuda())\n",
        "\t\ty = Variable(torch.squeeze(batch['label']).long().cuda())\n",
        "\t\ty_pred, h = net(X)\n",
        "\t\tloss = criterion(y_pred, y)\n",
        "\t\tif loss.cpu().detach().numpy() < 400:\n",
        "\t\t\tcount+=1\n",
        "\t\t\terror += loss.cpu().detach().numpy()\n",
        "\t\t\tlabels += list(y.cpu().detach().numpy())\n",
        "\t\t\tlogits += list(np.argmax(F.log_softmax(y_pred, 1).cpu().detach().numpy(), axis=1))\n",
        "\trecall = recall_score(labels, logits, average='micro')\n",
        "\tmetric_test.append([n_epoch, error/count, recall])\n",
        "\tprint(\"Test : epoch {} loss {} recall {}\".format(n_epoch, error/count, recall))\n",
        "\n",
        "\t#-------------------------------------------------------------------------------------------------------#\n",
        "\terror = 0\n",
        "\tcount = 0\n",
        "\tlabels = []\n",
        "\tlogits = []\n",
        "\tnet.train()\n",
        "\tfor i, batch in enumerate(TrainSample):\n",
        "\t\tX = Variable(batch['data'].cuda())\n",
        "\t\ty = Variable(torch.squeeze(batch['label']).long().cuda())\n",
        "\t\ty_pred, h = net(X)\n",
        "\t\t# print(h.sum(-1).cpu().data)\n",
        "\t\t# y_pred = torch.squeeze(y_) \n",
        "\t\toptimizer.zero_grad()\n",
        "\t\tloss = criterion(y_pred, y)\n",
        "\t\t# print(F.log_softmax(y_pred, 1).cpu().detach().numpy()[0])\n",
        "\n",
        "\t\tloss.backward()\n",
        "\t\toptimizer.step()\n",
        "\t\t# torch.save({'epoch': n_epoch + 1, 'state_dict': net.state_dict()},'/content/drive/My Drive/checkpoint.pth')\n",
        "\t\t# print(\"new\")\n",
        "\t\t# print(np.sum(X.cpu().detach().numpy(), axis=1))\n",
        "\t\t# for param in net.parameters():\n",
        "\t\t# \ttemp = param.grad.cpu().numpy()\n",
        "\t\t# \tprint(np.sum((temp == 0.0)*1, axis = 1)) \n",
        "\t\t# optimizer.zero_grad()\t\tprint(np.where(np.array(Vv) == 1)[0])\n",
        "\t\tcount +=1\n",
        "\t\terror += loss.cpu().detach().numpy()\n",
        "\t\tlabels += list(y.cpu().detach().numpy())\n",
        "\t\tlogits += list(np.argmax(F.log_softmax(y_pred, 1).cpu().detach().numpy(), axis=1))\n",
        "\tadjust_learning_rate(optimizer, lr, lr_decay)\n",
        "\trecall = recall_score(labels, logits, average='micro')\n",
        "\tmetric_train.append([n_epoch, error/count, recall])\n",
        "\t# torch.save({'epoch': epoch + 1, 'state_dict': net.state_dict()},'/content/drive/My Drive/checkpoint_{}.pth'.format(epoch + 1))\n",
        "\tprint(\"Train : epoch {} loss {} recall {}\".format(n_epoch, error/count, recall))\n",
        " \n",
        "np.save(\"graphTrain\", metric_train)\n",
        "np.save(\"graphTest\", metric_test)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "starting training!\n",
            "Test : epoch 0 loss 6.077556192803009 recall 0.0024132730015082957\n",
            "Train : epoch 0 loss 5.289526493461044 recall 0.05591240875912409\n",
            "Test : epoch 1 loss 4.148096358315936 recall 0.18265460030165911\n",
            "Train : epoch 1 loss 3.4060789699907654 recall 0.291021897810219\n",
            "Test : epoch 2 loss 3.313821630756875 recall 0.32450980392156864\n",
            "Train : epoch 2 loss 2.382193586340657 recall 0.45474452554744527\n",
            "Test : epoch 3 loss 3.379032871835338 recall 0.3625188536953243\n",
            "Train : epoch 3 loss 1.8731743141456887 recall 0.5383576642335767\n",
            "Test : epoch 4 loss 3.61194028871626 recall 0.3726998491704374\n",
            "Train : epoch 4 loss 1.5514504799136408 recall 0.6101094890510949\n",
            "Test : epoch 5 loss 3.9974812400067816 recall 0.37812971342383106\n",
            "Train : epoch 5 loss 1.2802376747131348 recall 0.6744525547445256\n",
            "Test : epoch 6 loss 4.3497106346635395 recall 0.3748868778280543\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-27-9d05a24e92c0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     50\u001b[0m         \u001b[0mlogits\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m         \u001b[0mnet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 52\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mTrainSample\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     53\u001b[0m                 \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mVariable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'data'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m                 \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mVariable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'label'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlong\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    343\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    344\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__next__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 345\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    346\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    347\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterable\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    383\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    384\u001b[0m         \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 385\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_fetcher\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    386\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    387\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jodeqfibNf8b",
        "colab_type": "code",
        "outputId": "b42fb0c3-640f-444b-97e9-ee74afe3d925",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "ls\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[0m\u001b[01;34msample_data\u001b[0m/\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K4V5sIBENtkk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from google.colab import files\n",
        "files.download('graph.npy') "
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}